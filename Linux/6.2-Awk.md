
# **Using the `awk` Command in Linux**

---

## **1. Learning Objectives**

By the end of this lesson, students will be able to:

* Understand what `awk` is and why itâ€™s used.
* Use `awk` to process and analyze text files.
* Select specific columns/fields from structured text.
* Perform basic calculations and formatting with `awk`.
* Apply `awk` in real-world tasks like log analysis and report generation.

---

## **2. Introduction to `awk`**

* **`awk`** is a text-processing tool that works by scanning each line of input and splitting it into fields.
* It is especially useful for **structured text** such as logs, CSV files, or system output.
* Syntax:

  ```bash
  awk 'pattern { action }' file
  ```
* By default, fields are separated by whitespace, but you can change the delimiter.

---

## **3. Basic Usage**

### **Print Entire File**

```bash
awk '{print}' file.txt
```

* Prints all lines (similar to `cat`).

### **Print Specific Columns**

```bash
awk '{print $1}' file.txt
```

* Prints the **first column** from each line.
* `$1` = first field, `$2` = second field, `$0` = whole line.

### **Example: `/etc/passwd`**

```bash
awk -F: '{print $1}' /etc/passwd
```

* Prints only usernames (first field before `:`).

---

## **4. Useful Options and Patterns**

| Feature | Description                      | Example                                           |
| ------- | -------------------------------- | ------------------------------------------------- |
| `-F`    | Set custom field separator       | `awk -F, '{print $2}' file.csv`                   |
| `NR`    | Current line number              | `awk '{print NR, $0}' file.txt`                   |
| `NF`    | Number of fields in current line | `awk '{print NF}' file.txt`                       |
| `BEGIN` | Action before processing lines   | `awk 'BEGIN {print "Start"} {print $0}' file.txt` |
| `END`   | Action after processing lines    | `awk 'END {print "Done"}' file.txt`               |

---

## **5. Conditions and Filters**

* You can add patterns to filter lines.

### **Example: Match Specific Value**

```bash
awk '$3 > 1000 {print $1, $3}' /etc/passwd
```

* Prints usernames and IDs where the third field (UID) is greater than 1000.

### **Example: Pattern Matching**

```bash
awk '/error/ {print $0}' logfile.txt
```

* Prints lines containing *error*.

---

## **6. Mathematical Operations**

* `awk` can do calculations on numeric fields.

### **Example: Sum Values**

```bash
awk '{sum += $2} END {print sum}' sales.txt
```

* Sums up the values in column 2.

### **Example: Average**

```bash
awk '{sum+=$3} END {print "Average:", sum/NR}' data.txt
```

* Computes the average of the third column.

---

## **7. Formatting Output**

* `awk` supports formatted printing like `printf`.

### **Example: Format Columns**

```bash
awk '{printf "%-10s %-10s\n", $1, $2}' data.txt
```

* Prints two columns, left-aligned in 10-character widths.

---

## **8. Real-World Examples**

1. **Show disk usage neatly**

```bash
df -h | awk '{print $1, $5}'
```

* Prints filesystem name and usage percentage.

2. **Extract IP addresses from Apache log**

```bash
awk '{print $1}' access.log | sort | uniq -c | sort -nr
```

* Counts unique IP addresses.

3. **Process CSV file**

```bash
awk -F, '{print $1, $3}' data.csv
```

* Extracts first and third fields from a CSV.

---

## **9. Practice Exercises**

1. Print the second column from `/etc/passwd`.
2. Print only lines in `/etc/passwd` where the shell is `/bin/bash`.
3. Using `-F:`, display usernames and their home directories from `/etc/passwd`.
4. From a CSV file `grades.csv`, calculate the average of the 3rd column.
5. Use `awk` with `df -h` to display only filesystems over 80% usage.
6. Write an `awk` command that prints the line number and the line itself.

---

## **10. Summary**

* `awk` is a **powerful text processing and reporting tool**.
* It works by splitting lines into fields and applying actions.
* Useful for system administration, data analysis, and log parsing.
* Supports conditions, loops, arithmetic, and formatted output.

---
